{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648d6b4f",
   "metadata": {},
   "source": [
    "# Snowpark For Python -- Advertising Spend and ROI Prediction\n",
    "\n",
    "### Objective\n",
    "In this session, we will train a Linear Regression model to predict future ROI (Return On Investment) of variable ad spend budgets across multiple channels including search, video, social media, and email using Snowpark for Python and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bfbd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType, StructField, DateType, Variant\n",
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit\n",
    "from snowflake.snowpark.version import VERSION\n",
    "# Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc71d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : utkarshdhande\n",
      "Role                        : ACCCOUNTADMIN\n",
      "Database                    : DASH_DB\n",
      "Schema                      : DASH_SCHEMA\n",
      "Warehouse                   : DASH_L\n",
      "Snowflake version           : 7.2.0\n",
      "Snowpark for Python version : 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : utkarshdhande'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : ACCCOUNTADMIN'.format(snowflake_environment[0][1]))\n",
    "print('Database                    : DASH_DB'.format(snowflake_environment[0][2]))\n",
    "print('Schema                      : DASH_SCHEMA'.format(snowflake_environment[0][3]))\n",
    "print('Warehouse                   : DASH_L'.format(snowflake_environment[0][5]))\n",
    "print('Snowflake version           : 7.2.0'.format(snowflake_environment[0][4]))\n",
    "print('Snowpark for Python version : 1.0.0'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "212a3660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': ['SELECT  *  FROM (campaign_spend)'], 'post_actions': []}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_df_spend = session.table('campaign_spend')\n",
    "snow_df_spend.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47096641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n",
      "|\"CAMPAIGN\"              |\"CHANNEL\"      |\"DATE\"      |\"TOTAL_CLICKS\"  |\"TOTAL_COST\"  |\"ADS_SERVED\"  |\n",
      "------------------------------------------------------------------------------------------------------\n",
      "|winter_sports           |video          |2012-06-03  |213             |1762          |426           |\n",
      "|sports_across_cultures  |video          |2012-06-02  |87              |678           |157           |\n",
      "|building_community      |search_engine  |2012-06-03  |66              |471           |134           |\n",
      "|world_series            |social_media   |2017-12-28  |72              |591           |149           |\n",
      "|winter_sports           |email          |2018-02-09  |252             |1841          |473           |\n",
      "|spring_break            |video          |2017-11-14  |162             |1155          |304           |\n",
      "|nba_finals              |email          |2017-11-22  |68              |480           |134           |\n",
      "|winter_sports           |social_media   |2018-03-10  |227             |1797          |454           |\n",
      "|spring_break            |search_engine  |2017-08-30  |150             |1226          |302           |\n",
      "|uefa                    |video          |2017-09-30  |81              |701           |168           |\n",
      "------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[QueryRecord(query_id='01aa885f-0000-1239-0000-00005135555d', sql_text='SELECT  *  FROM campaign_spend LIMIT 10')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action sends the DF SQL for execution\n",
    "# Note: history object provides the query ID which can be helpful for debugging as well as the SQL query executed on the server\n",
    "with session.query_history() as history:\n",
    "    snow_df_spend.show()\n",
    "history.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f81242",
   "metadata": {},
   "source": [
    "### Total Spend per Channel per Month\n",
    "Let's transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25cb6fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"CHANNEL\"      |\"TOTAL_COST\"  |\n",
      "---------------------------------------------------\n",
      "|2012    |5        |search_engine  |516431        |\n",
      "|2012    |5        |video          |516729        |\n",
      "|2012    |5        |email          |517208        |\n",
      "|2012    |5        |social_media   |517618        |\n",
      "|2012    |6        |video          |501098        |\n",
      "|2012    |6        |search_engine  |506497        |\n",
      "|2012    |6        |social_media   |504679        |\n",
      "|2012    |6        |email          |501947        |\n",
      "|2012    |7        |search_engine  |522780        |\n",
      "|2012    |7        |email          |518405        |\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stats per Month per Channel\n",
    "snow_df_spend_per_channel = snow_df_spend.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n",
    "    with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n",
    "\n",
    "snow_df_spend_per_channel.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7aae8",
   "metadata": {},
   "source": [
    "#### Pivot on Channel\n",
    "Let's further transform the campaign spend data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions. This transformation will enable us to join with the revenue table such that we will have our input features and target variable in a single table for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33e9407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\n",
      "---------------------------------------------------------------------------\n",
      "|2012    |5        |516431           |517618          |516729   |517208   |\n",
      "|2012    |6        |506497           |504679          |501098   |501947   |\n",
      "|2012    |7        |522780           |521395          |522762   |518405   |\n",
      "|2012    |8        |519959           |520537          |520685   |521584   |\n",
      "|2012    |9        |507211           |507404          |511364   |507363   |\n",
      "|2012    |10       |518942           |520863          |522768   |519950   |\n",
      "|2012    |11       |505715           |505221          |505292   |503748   |\n",
      "|2012    |12       |520148           |520711          |521427   |520724   |\n",
      "|2013    |1        |522151           |518635          |520583   |521167   |\n",
      "|2013    |2        |467736           |474679          |469856   |469784   |\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_per_month = snow_df_spend_per_channel.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n",
    "snow_df_spend_per_month = snow_df_spend_per_month.select(\n",
    "    col(\"YEAR\"),\n",
    "    col(\"MONTH\"),\n",
    "    col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n",
    "    col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n",
    "    col(\"'video'\").as_(\"VIDEO\"),\n",
    "    col(\"'email'\").as_(\"EMAIL\")\n",
    ")\n",
    "snow_df_spend_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923419c",
   "metadata": {},
   "source": [
    "#### Total Revenue per Month\n",
    "Now let's load revenue table and transform the data into revenue per year/month using group_by and agg() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a59e475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"REVENUE\"   |\n",
      "---------------------------------\n",
      "|2012    |5        |3264300.11  |\n",
      "|2012    |6        |3208482.33  |\n",
      "|2012    |7        |3311966.98  |\n",
      "|2012    |8        |3311752.81  |\n",
      "|2012    |9        |3208563.06  |\n",
      "|2012    |10       |3334028.46  |\n",
      "|2012    |11       |3185894.64  |\n",
      "|2012    |12       |3334570.96  |\n",
      "|2013    |1        |3316455.44  |\n",
      "|2013    |2        |2995042.21  |\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_revenue = session.table('monthly_revenue')\n",
    "snow_df_revenue_per_month = snow_df_revenue.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n",
    "snow_df_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61e944",
   "metadata": {},
   "source": [
    "#### Join Total Spend and Total Revenue per Month\n",
    "Next let's join this revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81ffec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"REVENUE\"   |\n",
      "----------------------------------------------------------------------------------------\n",
      "|2012    |5        |516431           |517618          |516729   |517208   |3264300.11  |\n",
      "|2012    |6        |506497           |504679          |501098   |501947   |3208482.33  |\n",
      "|2012    |7        |522780           |521395          |522762   |518405   |3311966.98  |\n",
      "|2012    |8        |519959           |520537          |520685   |521584   |3311752.81  |\n",
      "|2012    |9        |507211           |507404          |511364   |507363   |3208563.06  |\n",
      "|2012    |10       |518942           |520863          |522768   |519950   |3334028.46  |\n",
      "|2012    |11       |505715           |505221          |505292   |503748   |3185894.64  |\n",
      "|2012    |12       |520148           |520711          |521427   |520724   |3334570.96  |\n",
      "|2013    |1        |522151           |518635          |520583   |521167   |3316455.44  |\n",
      "|2013    |2        |467736           |474679          |469856   |469784   |2995042.21  |\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_and_revenue_per_month = snow_df_spend_per_month.join(snow_df_revenue_per_month, [\"YEAR\",\"MONTH\"])\n",
    "snow_df_spend_and_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc090a3",
   "metadata": {},
   "source": [
    "### >>>>>>>>>> Examine Snowpark DataFrame Query and Execution Plan <<<<<<<<<<\n",
    "Snowpark makes is really convenient to look at the DataFrame query and execution plan using explain() Snowpark DataFrame function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9879ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------DATAFRAME EXECUTION PLAN----------\n",
      "Query List:\n",
      "1.\n",
      "SELECT  *  FROM (( SELECT \"YEAR\" AS \"YEAR\", \"MONTH\" AS \"MONTH\", \"SEARCH_ENGINE\" AS \"SEARCH_ENGINE\", \"SOCIAL_MEDIA\" AS \"SOCIAL_MEDIA\", \"VIDEO\" AS \"VIDEO\", \"EMAIL\" AS \"EMAIL\" FROM ( SELECT \"YEAR\", \"MONTH\", \"'search_engine'\" AS \"SEARCH_ENGINE\", \"'social_media'\" AS \"SOCIAL_MEDIA\", \"'video'\" AS \"VIDEO\", \"'email'\" AS \"EMAIL\" FROM ( SELECT  *  FROM ( SELECT  *  FROM ( SELECT  *  FROM ( SELECT \"YEAR(DATE)\" AS \"YEAR\", \"MONTH(DATE)\" AS \"MONTH\", \"CHANNEL\", \"TOTAL_COST\" FROM ( SELECT year(\"DATE\") AS \"YEAR(DATE)\", month(\"DATE\") AS \"MONTH(DATE)\", \"CHANNEL\", sum(\"TOTAL_COST\") AS \"TOTAL_COST\" FROM ( SELECT  *  FROM campaign_spend) GROUP BY year(\"DATE\"), month(\"DATE\"), \"CHANNEL\")) ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST) PIVOT (sum(\"TOTAL_COST\") FOR \"CHANNEL\" IN ('search_engine', 'social_media', 'video', 'email'))) ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST))) AS SNOWPARK_TEMP_TABLE_QTNRD0ZAOO INNER JOIN ( SELECT \"YEAR\" AS \"YEAR\", \"MONTH\" AS \"MONTH\", \"REVENUE\" AS \"REVENUE\" FROM ( SELECT \"YEAR\", \"MONTH\", \"SUM(REVENUE)\" AS \"REVENUE\" FROM ( SELECT  *  FROM ( SELECT \"YEAR\", \"MONTH\", sum(\"REVENUE\") AS \"SUM(REVENUE)\" FROM ( SELECT  *  FROM monthly_revenue) GROUP BY \"YEAR\", \"MONTH\") ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST))) AS SNOWPARK_TEMP_TABLE_YG1R7TTL2B USING (YEAR, MONTH))\n",
      "Logical Execution Plan:\n",
      "GlobalStats:\n",
      "    partitionsTotal=2\n",
      "    partitionsAssigned=2\n",
      "    bytesAssigned=1731072\n",
      "Operations:\n",
      "1:0     ->Result  EXTRACT(year from CAMPAIGN_SPEND.DATE), EXTRACT(month from CAMPAIGN_SPEND.DATE), 'search_engine', 'social_media', 'video', 'email', SUM(MONTHLY_REVENUE.REVENUE)  \n",
      "1:1          ->InnerJoin  joinKey: (EXTRACT(month from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.MONTH) AND (EXTRACT(year from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.YEAR)  \n",
      "1:2               ->Sort  EXTRACT(year from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST, EXTRACT(month from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST  \n",
      "1:3                    ->Pivot  'search_engine', 'social_media', 'video', 'email'  \n",
      "1:4                         ->Sort  EXTRACT(year from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST, EXTRACT(month from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST  \n",
      "1:5                              ->Aggregate  aggExprs: [SUM(CAMPAIGN_SPEND.TOTAL_COST)], groupKeys: [EXTRACT(year from CAMPAIGN_SPEND.DATE), EXTRACT(month from CAMPAIGN_SPEND.DATE), CAMPAIGN_SPEND.CHANNEL]  \n",
      "1:6                                   ->TableScan  DASH_DB.DASH_SCHEMA.CAMPAIGN_SPEND  CHANNEL, DATE, TOTAL_COST  {partitionsTotal=1, partitionsAssigned=1, bytesAssigned=1728512}\n",
      "1:7               ->Sort  MONTHLY_REVENUE.YEAR ASC NULLS FIRST, MONTHLY_REVENUE.MONTH ASC NULLS FIRST  \n",
      "1:8                    ->JoinFilter  joinKey: (EXTRACT(month from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.MONTH) AND (EXTRACT(year from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.YEAR)  \n",
      "1:9                         ->TableScan  DASH_DB.DASH_SCHEMA.MONTHLY_REVENUE  YEAR, MONTH, REVENUE  {partitionsTotal=1, partitionsAssigned=1, bytesAssigned=2560}\n",
      "\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_and_revenue_per_month.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d3ce2",
   "metadata": {},
   "source": [
    "### Model Training in Snowflake\n",
    "#### Features and Target\n",
    "At this point we are ready to perform the following actions to save features and target for model training.\n",
    "\n",
    "Delete rows with missing values\n",
    "Exclude columns we don't need for modeling\n",
    "Save features into a Snowflake table called MARKETING_BUDGETS_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a0f33a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"REVENUE\"   |\n",
      "---------------------------------------------------------------------\n",
      "|516431           |517618          |516729   |517208   |3264300.11  |\n",
      "|506497           |504679          |501098   |501947   |3208482.33  |\n",
      "|522780           |521395          |522762   |518405   |3311966.98  |\n",
      "|519959           |520537          |520685   |521584   |3311752.81  |\n",
      "|507211           |507404          |511364   |507363   |3208563.06  |\n",
      "|518942           |520863          |522768   |519950   |3334028.46  |\n",
      "|505715           |505221          |505292   |503748   |3185894.64  |\n",
      "|520148           |520711          |521427   |520724   |3334570.96  |\n",
      "|522151           |518635          |520583   |521167   |3316455.44  |\n",
      "|467736           |474679          |469856   |469784   |2995042.21  |\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete rows with missing values\n",
    "snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.dropna()\n",
    "\n",
    "# Exclude columns we don't need for modeling\n",
    "snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.drop(['YEAR','MONTH'])\n",
    "\n",
    "# Save features into a Snowflake table call MARKETING_BUDGETS_FEATURES\n",
    "snow_df_spend_and_revenue_per_month.write.mode('overwrite').save_as_table('MARKETING_BUDGETS_FEATURES')\n",
    "snow_df_spend_and_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb3ec70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': ['SELECT  *  FROM (MARKETING_BUDGETS_FEATURES)'],\n",
       " 'post_actions': []}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = session.table('MARKETING_BUDGETS_FEATURES')\n",
    "tst.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd115956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"REVENUE\"   |\n",
      "---------------------------------------------------------------------\n",
      "|516431           |517618          |516729   |517208   |3264300.11  |\n",
      "|506497           |504679          |501098   |501947   |3208482.33  |\n",
      "|522780           |521395          |522762   |518405   |3311966.98  |\n",
      "|519959           |520537          |520685   |521584   |3311752.81  |\n",
      "|507211           |507404          |511364   |507363   |3208563.06  |\n",
      "|518942           |520863          |522768   |519950   |3334028.46  |\n",
      "|505715           |505221          |505292   |503748   |3185894.64  |\n",
      "|520148           |520711          |521427   |520724   |3334570.96  |\n",
      "|522151           |518635          |520583   |521167   |3316455.44  |\n",
      "|467736           |474679          |469856   |469784   |2995042.21  |\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[QueryRecord(query_id='01aa885f-0000-1239-0000-00005135557d', sql_text='SELECT  *  FROM MARKETING_BUDGETS_FEATURES LIMIT 10')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with session.query_history() as history:\n",
    "    tst.show()\n",
    "history.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2358e",
   "metadata": {},
   "source": [
    "#### Python function to train a Linear Regression model using scikit-learn\n",
    "Let's create a Python function that uses scikit-learn and other packages which are already included in Snowflake Anaconda channel and therefore available on the server-side when executing the Python function as a Stored Procedure running in Snowflake.\n",
    "\n",
    "This function takes the following as parameters:\n",
    "\n",
    "session: Snowflake Session object.\n",
    "features_table: Name of the table that holds the features and target variable.\n",
    "number_of_folds: Number of cross validation folds used in GridSearchCV.\n",
    "polynomial_features_degress: PolynomialFeatures as a preprocessing step.\n",
    "train_accuracy_threshold: Accuracy thresholds for train dataset. This values is used to determine if the model should be saved.\n",
    "test_accuracy_threshold: Accuracy thresholds for test dataset. This values is used to determine if the model should be saved.\n",
    "save_model: Boolean that determines if the model should be saved provided the accuracy thresholds are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28e65f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_revenue_prediction_model(\n",
    "    session: Session, \n",
    "    features_table: str, \n",
    "    number_of_folds: int, \n",
    "    polynomial_features_degrees: int, \n",
    "    train_accuracy_threshold: float, \n",
    "    test_accuracy_threshold: float, \n",
    "    save_model: bool) -> Variant:\n",
    "    \n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "    import os\n",
    "    from joblib import dump\n",
    "\n",
    "    # Load features\n",
    "    df = session.table(features_table).to_pandas()\n",
    "\n",
    "    # Preprocess the Numeric columns\n",
    "    # We apply PolynomialFeatures and StandardScaler preprocessing steps to the numeric columns\n",
    "    # NOTE: High degrees can cause overfitting.\n",
    "    numeric_features = ['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL']\n",
    "    numeric_transformer = Pipeline(steps=[('poly',PolynomialFeatures(degree = polynomial_features_degrees)),('scaler', StandardScaler())])\n",
    "\n",
    "    # Combine the preprocessed step together using the Column Transformer module\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "    # The next step is the integrate the features we just preprocessed with our Machine Learning algorithm to enable us to build a model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', LinearRegression())])\n",
    "    parameteres = {}\n",
    "\n",
    "    X = df.drop('REVENUE', axis = 1)\n",
    "    y = df['REVENUE']\n",
    "\n",
    "    # Split dataset into training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "    # Use GridSearch to find the best fitting model based on number_of_folds folds\n",
    "    model = GridSearchCV(pipeline, param_grid=parameteres, cv=number_of_folds)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_r2_score = model.score(X_train, y_train)\n",
    "    test_r2_score = model.score(X_test, y_test)\n",
    "\n",
    "    model_saved = False\n",
    "\n",
    "    if save_model:\n",
    "        if train_r2_score >= train_accuracy_threshold and test_r2_score >= test_accuracy_threshold:\n",
    "            # Upload trained model to a stage\n",
    "            model_output_dir = '/tmp'\n",
    "            model_file = os.path.join(model_output_dir, 'model.joblib')\n",
    "            dump(model, model_file)\n",
    "            session.file.put(model_file,\"@dash_models\",overwrite=True)\n",
    "            model_saved = True\n",
    "\n",
    "    # Return model R2 score on train and test data\n",
    "    return {\"R2 score on Train\": train_r2_score,\n",
    "            \"R2 threshold on Train\": train_accuracy_threshold,\n",
    "            \"R2 score on Test\": test_r2_score,\n",
    "            \"R2 threshold on Test\": test_accuracy_threshold,\n",
    "            \"Model saved\": model_saved}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e1192",
   "metadata": {},
   "source": [
    "### Test Python function before deploying it as a Stored Procedure on Snowflake\n",
    "Since we're in test mode, we will set save_model = False so that the model is not saved just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e54c7ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R2 score on Train': 0.9954552822793987,\n",
       " 'R2 threshold on Train': 0.85,\n",
       " 'R2 score on Test': 0.8817971097765028,\n",
       " 'R2 threshold on Test': 0.85,\n",
       " 'Model saved': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validaton_folds = 10\n",
    "polynomial_features_degrees = 2\n",
    "train_accuracy_threshold = 0.85\n",
    "test_accuracy_threshold = 0.85\n",
    "save_model = False\n",
    "\n",
    "train_revenue_prediction_model(\n",
    "    session,\n",
    "    'MARKETING_BUDGETS_FEATURES',\n",
    "    cross_validaton_folds,\n",
    "    polynomial_features_degrees,\n",
    "    train_accuracy_threshold,\n",
    "    test_accuracy_threshold,\n",
    "    save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a3f58",
   "metadata": {},
   "source": [
    "### Create Stored Procedure to deploy model training code on Snowflake\n",
    "Assuming the testing is complete and we're satisfied with the model, let's register the model training Python function as a Snowpark Python Stored Procedure by supplying the packages (snowflake-snowpark-python,scikit-learn, and joblib) it will need and use during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e752987c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x1e1eb718190>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(\n",
    "    func=train_revenue_prediction_model,\n",
    "    name=\"train_revenue_prediction_model\",\n",
    "    packages=['snowflake-snowpark-python','scikit-learn','joblib'],\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@dash_sprocs\",\n",
    "    replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a88da3",
   "metadata": {},
   "source": [
    "### >>>>>>>>>> Examine Query History in Snowsight <<<<<<<<<<\n",
    "Execute Stored Procedure to train model and deploy it on Snowflake\n",
    "Now we're ready to train the model and save it onto a Snowflake stage so let's set save_model = True and run/execute the Stored Procedure using session.call() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d582f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Model saved\": true,\n",
      "  \"R2 score on Test\": 0.8817971097765088,\n",
      "  \"R2 score on Train\": 0.9954552822793987,\n",
      "  \"R2 threshold on Test\": 0.85,\n",
      "  \"R2 threshold on Train\": 0.85\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cross_validaton_folds = 10\n",
    "polynomial_features_degrees = 2\n",
    "train_accuracy_threshold = 0.85\n",
    "test_accuracy_threshold = 0.85\n",
    "save_model = True\n",
    "\n",
    "print(session.call('train_revenue_prediction_model',\n",
    "                    'MARKETING_BUDGETS_FEATURES',\n",
    "                    cross_validaton_folds,\n",
    "                    polynomial_features_degrees,\n",
    "                    train_accuracy_threshold,\n",
    "                    test_accuracy_threshold,\n",
    "                    save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5298a",
   "metadata": {},
   "source": [
    "### Create Stored Procedure to deploy model training code on Snowflake\n",
    "Assuming the testing is complete and we're satisfied with the model, let's register the model training Python function as a Snowpark Python Stored Procedure by supplying the packages (snowflake-snowpark-python,scikit-learn, and joblib) it will need and use during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ec96479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x1e1eb74d0d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(\n",
    "    func=train_revenue_prediction_model,\n",
    "    name=\"train_revenue_prediction_model\",\n",
    "    packages=['snowflake-snowpark-python','scikit-learn','joblib'],\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@dash_sprocs\",\n",
    "    replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47027bd7",
   "metadata": {},
   "source": [
    "### >>>>>>>>>> Examine Query History in Snowsight <<<<<<<<<<\n",
    "Execute Stored Procedure to train model and deploy it on Snowflake\n",
    "Now we're ready to train the model and save it onto a Snowflake stage so let's set save_model = True and run/execute the Stored Procedure using session.call() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82c8dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Model saved\": true,\n",
      "  \"R2 score on Test\": 0.8817971097765088,\n",
      "  \"R2 score on Train\": 0.9954552822793987,\n",
      "  \"R2 threshold on Test\": 0.85,\n",
      "  \"R2 threshold on Train\": 0.85\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cross_validaton_folds = 10\n",
    "polynomial_features_degrees = 2\n",
    "train_accuracy_threshold = 0.85\n",
    "test_accuracy_threshold = 0.85\n",
    "save_model = True\n",
    "\n",
    "print(session.call('train_revenue_prediction_model',\n",
    "                    'MARKETING_BUDGETS_FEATURES',\n",
    "                    cross_validaton_folds,\n",
    "                    polynomial_features_degrees,\n",
    "                    train_accuracy_threshold,\n",
    "                    test_accuracy_threshold,\n",
    "                    save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7746d5",
   "metadata": {},
   "source": [
    "### >>>>>>>>>> Examine Query History in Snowsight <<<<<<<<<<\n",
    "Create Scalar User-Defined Function (UDF) for inference\n",
    "Now to deploy this model for inference, let's create and register a Snowpark Python UDF and add the trained model as a dependency. Once registered, getting new predictions is as simple as calling the function by passing in data.\n",
    "\n",
    "NOTE: Scalar UDFs operate on a single row / set of data points and are great for online inference in real-time. And this UDF is called from the Streamlit App. See Snowpark_Streamlit_Revenue_Prediction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3a51f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "# Add trained model and Python packages from Snowflake Anaconda channel available on the server-side as UDF dependencies\n",
    "session.add_import('@dash_models/model.joblib.gz')\n",
    "session.add_packages('pandas','joblib','scikit-learn==1.1.1')\n",
    "\n",
    "@udf(name='predict_roi',session=session,replace=True,is_permanent=True,stage_location='@dash_udfs')\n",
    "def predict_roi(budget_allocations: list) -> float:\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    from joblib import load\n",
    "    import sklearn\n",
    "\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    \n",
    "    model_file = import_dir + 'model.joblib.gz'\n",
    "    model = load(model_file)\n",
    "            \n",
    "    features = ['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL']\n",
    "    df = pd.DataFrame([budget_allocations], columns=features)\n",
    "    roi = abs(model.predict(df)[0])\n",
    "    return roi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf75422",
   "metadata": {},
   "source": [
    "### Call Scalar User-Defined Function (UDF) for inference on new data\n",
    "Once the UDF is registered, getting new predictions is as simple as calling the call_udf() Snowpark Python function and passing in new datapoints.\n",
    "\n",
    "Let's create a SnowPark DataFrame with some sample data and call the UDF to get new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a574c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"PREDICTED_ROI\"     |\n",
      "-----------------------------------------------------------------------------\n",
      "|500000           |500000          |500000   |500000   |3179613.166194177   |\n",
      "|8500             |9500            |2000     |500      |189866.83304722887  |\n",
      "|250000           |250000          |200000   |450000   |4072491.441668165   |\n",
      "-----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = session.create_dataframe([[250000,250000,200000,450000],[500000,500000,500000,500000],[8500,9500,2000,500]], \n",
    "                                    schema=['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL'])\n",
    "test_df.select(\n",
    "    'SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL', \n",
    "    call_udf(\"predict_roi\", \n",
    "    array_construct(col(\"SEARCH_ENGINE\"), col(\"SOCIAL_MEDIA\"), col(\"VIDEO\"), col(\"EMAIL\"))).as_(\"PREDICTED_ROI\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98611f84",
   "metadata": {},
   "source": [
    "### Create Vectorized User-Defined Function (UDF) using Batch API for inference\n",
    "Here we will leverage the Python UDF Batch API to create a vectorized UDF which takes a Pandas Dataframe as input. This means that each call to the UDF receives a set/batch of rows compared to a Scalar UDF which gets one row as input.\n",
    "\n",
    "First we will create a helper function load_model() that uses cachetools to make sure we only load the model once followed by batch_predict_roi() function that does the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8d4a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "import cachetools\n",
    "from snowflake.snowpark.types import PandasSeries, PandasDataFrame\n",
    "\n",
    "# Add trained model and Python packages from Snowflake Anaconda channel available on the server-side as UDF dependencies\n",
    "session.add_import('@dash_models/model.joblib.gz')\n",
    "session.add_packages('pandas','joblib','scikit-learn','cachetools')\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def load_model(filename):\n",
    "    import joblib\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n",
    "\n",
    "@udf(name='batch_predict_roi',session=session,replace=True,is_permanent=True,stage_location='@dash_udfs')\n",
    "def batch_predict_roi(budget_allocations_df: PandasDataFrame[int, int, int, int]) -> PandasSeries[float]:\n",
    "    import sklearn\n",
    "    budget_allocations_df.columns = ['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL']\n",
    "    model = load_model('model.joblib.gz')\n",
    "    return abs(model.predict(budget_allocations_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8c8fa",
   "metadata": {},
   "source": [
    "### Call Vectorized User-Defined Function (UDF) using Batch API for inference on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25e466d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"PREDICTED_ROI\"     |\n",
      "-----------------------------------------------------------------------------\n",
      "|250000           |250000          |200000   |450000   |4072491.441668165   |\n",
      "|500000           |500000          |500000   |500000   |3179613.166194177   |\n",
      "|8500             |9500            |2000     |500      |189866.83304722887  |\n",
      "-----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select(\n",
    "    'SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL', \n",
    "    call_udf(\"batch_predict_roi\", \n",
    "    col(\"SEARCH_ENGINE\"), col(\"SOCIAL_MEDIA\"), col(\"VIDEO\"), col(\"EMAIL\")).as_(\"PREDICTED_ROI\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c0771",
   "metadata": {},
   "source": [
    "### Automate Data Pipeline and Model (re)Training using Snowflake Tasks\n",
    "We can also optionally create Snowflake (Serverless or User-managed) Tasks to automate data pipelining and (re)training of the model on a set schedule.\n",
    "\n",
    "NOTE: Creating tasks using Snowpark Python API (instead of SQL) is on the roadmap. Stay tuned! Or, follow me on [Twitter]((https://twitter.com/iamontheinet)) to get the news before anyone else :)_\n",
    "\n",
    "#### Create Python Function for Data Pipeline and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "844cf49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline_feature_engineering(session: Session) -> str:\n",
    "\n",
    "  # DATA TRANSFORMATIONS\n",
    "  # Perform the following actions to transform the data\n",
    "\n",
    "  # Load the campaign spend data\n",
    "  snow_df_spend = session.table('campaign_spend')\n",
    "\n",
    "  # Transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions\n",
    "  snow_df_spend_per_channel = snow_df_spend.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n",
    "      with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n",
    "\n",
    "  # Transform the data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions\n",
    "  snow_df_spend_per_month = snow_df_spend_per_channel.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n",
    "  snow_df_spend_per_month = snow_df_spend_per_month.select(\n",
    "      col(\"YEAR\"),\n",
    "      col(\"MONTH\"),\n",
    "      col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n",
    "      col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n",
    "      col(\"'video'\").as_(\"VIDEO\"),\n",
    "      col(\"'email'\").as_(\"EMAIL\")\n",
    "  )\n",
    "\n",
    "  # Load revenue table and transform the data into revenue per year/month using group_by and agg() functions\n",
    "  snow_df_revenue = session.table('monthly_revenue')\n",
    "  snow_df_revenue_per_month = snow_df_revenue.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n",
    "\n",
    "  # Join revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training\n",
    "  snow_df_spend_and_revenue_per_month = snow_df_spend_per_month.join(snow_df_revenue_per_month, [\"YEAR\",\"MONTH\"])\n",
    "\n",
    "  # SAVE FEATURES And TARGET\n",
    "  # Perform the following actions to save features and target for model training\n",
    "\n",
    "  # Delete rows with missing values\n",
    "  snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.dropna()\n",
    "\n",
    "  # Exclude columns we don't need for modeling\n",
    "  snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.drop(['YEAR','MONTH'])\n",
    "\n",
    "  # Save features into a Snowflake table call MARKETING_BUDGETS_FEATURES\n",
    "  snow_df_spend_and_revenue_per_month.write.mode('overwrite').save_as_table('MARKETING_BUDGETS_FEATURES')\n",
    "\n",
    "  return \"SUCCESS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3fa5a7",
   "metadata": {},
   "source": [
    "### Create Stored Procedure to deploy data pipelining feature engineeering code on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58be7db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x1e1e4f189d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(\n",
    "    func=data_pipeline_feature_engineering,\n",
    "    name=\"data_pipeline_feature_engineering\",\n",
    "    packages=['snowflake-snowpark-python'],\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@dash_sprocs\",\n",
    "    replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4d017",
   "metadata": {},
   "source": [
    "### Execute Stored Procedure to deploy data pipelining feature engineeering code on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1602c7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "print(session.call('data_pipeline_feature_engineering'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d39ac",
   "metadata": {},
   "source": [
    "### Create Root/Parent Snowflake Task: Data pipelining and feature engineeering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73eaa2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Task DATA_PIPELINE_FEATURE_ENGINEERING_TASK successfully created.')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_data_pipeline_feature_engineering_task = \"\"\"\n",
    "CREATE OR REPLACE TASK data_pipeline_feature_engineering_task\n",
    "    WAREHOUSE = 'DASH_L'\n",
    "    SCHEDULE  = '1 MINUTE'\n",
    "AS\n",
    "    CALL data_pipeline_feature_engineering()\n",
    "\"\"\"\n",
    "session.sql(create_data_pipeline_feature_engineering_task).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36728967",
   "metadata": {},
   "source": [
    "### Create Child/Dependent Snowflake Task: Model training on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfe442ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Task MODEL_TRAINING_TASK successfully created.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_training_task = \"\"\"\n",
    "CREATE OR REPLACE TASK model_training_task\n",
    "    WAREHOUSE = 'DASH_L'\n",
    "    AFTER data_pipeline_feature_engineering_task\n",
    "AS\n",
    "    CALL train_revenue_prediction_model('MARKETING_BUDGETS_FEATURES',10,2,0.85,0.85,True)\n",
    "\"\"\"\n",
    "session.sql(create_model_training_task).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57a2ef",
   "metadata": {},
   "source": [
    "### Resume Tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56e889f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"alter task model_training_task resume\").collect()\n",
    "session.sql(\"alter task data_pipeline_feature_engineering_task resume\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33a691",
   "metadata": {},
   "source": [
    "### Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0dfb0b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"alter task data_pipeline_feature_engineering_task suspend\").collect()\n",
    "session.sql(\"alter task model_training_task suspend\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949895ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
